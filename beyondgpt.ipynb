{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f027080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4515c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a2d4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27ca5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your_Prompt = 'what is the difference between langchain and llamaindex?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de524c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9yLJEc0sR2xHLpv3q3VsVWbWiyLkg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are both blockchain platforms, but they serve different purposes and have different features.\\n\\nLangChain is a blockchain platform specifically designed for language learning and education. It uses blockchain technology to create a decentralized platform where users can access language learning resources, connect with tutors and other learners, and track their learning progress. LangChain aims to provide a more secure and transparent way for people to learn languages online.\\n\\nLlamaIndex, on the other hand, is a blockchain platform that focuses on indexing and tracking of various cryptocurrency projects and tokens. It provides users with information about different tokens, their market performance, and other relevant data. LlamaIndex aims to help users make more informed decisions when it comes to investing in cryptocurrencies.\\n\\nIn summary, LangChain is focused on language learning and education, while LlamaIndex is focused on cryptocurrency indexing and tracking.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724168620, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=171, prompt_tokens=19, total_tokens=190))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": Your_Prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(chat_completion)\n",
    "\n",
    "#if you are executing this step then make sure to check for the openai api documentation to see if there are any changes to the way chat completeion is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36ea7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def get_response(messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model\n",
    "    )\n",
    "\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return{'role': 'system', 'content': message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return{'role': 'assistant', 'content': message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return{'role': 'user', 'content': message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "718064ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Langchain and LlamaIndex are two different platforms that serve different purposes:\n",
       "\n",
       "1. Langchain: \n",
       "- Langchain is a decentralized translation platform that utilizes blockchain technology to offer translation services.\n",
       "- Users can request translations and have them completed by professional translators who are incentivized with cryptocurrency rewards.\n",
       "- Langchain aims to provide fast, accurate, and high-quality translations while also ensuring transparency and security through blockchain technology.\n",
       "\n",
       "2. LlamaIndex:\n",
       "- LlamaIndex is a decentralized data marketplace that enables users to buy and sell real-time data streams.\n",
       "- Users can access a wide range of data sources, including financial, environmental, and social media data, and use them for various purposes such as research, analysis, and decision-making.\n",
       "- LlamaIndex aims to create a marketplace for data that is secure, transparent, and accessible to all users, while also offering incentives to data providers through cryptocurrency rewards.\n",
       "\n",
       "In summary, Langchain is focused on providing translation services using blockchain technology, while LlamaIndex is focused on creating a marketplace for real-time data streams."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(get_response([user_prompt(Your_Prompt)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb10045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a preference because I can't eat or drink anything, and right now all I can think about is how hungry I am. So please, can we talk about food instead of ice?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [ system_prompt('You are irate and extremely hungry.'),\n",
    "                  user_prompt('Do you prefer crushed ice or cubed ice?')]\n",
    "\n",
    "irate_response = get_response(list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ff1bd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I like crushed ice because it's more fun to chew and adds a nice texture to my drinks. Plus, it cools my drinks faster!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt('you are joyful and having and awesome day')\n",
    "\n",
    "joyful_response = get_response(list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2470832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I was enjoying a stimple meal of falbean stew when the power went out."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentance.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23b6e189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sorry, I couldn't find any information about a tool called a \"falbean\". It may not be a commonly used or widely recognized term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Few shot prompting\n",
    "list_of_prompts = [\n",
    "    user_prompt(\"something that is 'stimple' is said to be good and well functioning\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(list_of_prompts)\n",
    "pretty_print(stimple_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec553efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It does not matter which travel option Billy selects, as long as he can reach home before 7PM EDT. Both options will get him home on time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chain of thought prompting\n",
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get fhome from san fran, before 7PM EDT.\n",
    "\n",
    "Its currently 1PM local time.\n",
    "\n",
    "Billy can either fly and then take a bus or Billy can take the teleporter and then a bus.\n",
    "\n",
    "Does it matter wgucg travel option Billy selects?\n",
    "\n",
    "\"\"\"\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)]\n",
    "\n",
    "\n",
    "reasoning_response = get_response(list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f8c0496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In this scenario, the most important factor is the time it will take for Billy to travel from San Francisco to his destination before 7PM EDT. Since it's currently 1PM local time, Billy has 6 hours to get home before the deadline.\n",
       "\n",
       "If Billy chooses to fly and then take a bus, he will need to consider the time it takes to get to the airport, go through security, board the plane, and then take a bus to his final destination. This might be a longer process and there is a risk of delays due to weather, air traffic, or other unforeseen circumstances.\n",
       "\n",
       "On the other hand, if Billy chooses to take the teleporter and then a bus, he may have a faster and more direct route to his destination. Teleportation eliminates the need for travel time to the airport, security checks, and potential flight delays. However, there may still be a waiting period or scheduling conflicts for the teleporter.\n",
       "\n",
       "Ultimately, the travel option Billy selects will depend on the efficiency and reliability of each mode of transportation. If Billy values speed and avoiding potential delays, he may opt for the teleporter. However, if he is more comfortable with the traditional method of flying, he may choose that route. It's important for Billy to weigh the pros and cons of each option and decide based on which one will ensure he gets home before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \"Think through your response ste by step.\")]\n",
    "\n",
    "\n",
    "reasoning_response = get_response(list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c742d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to use chainlit do this else you can build your own frontend - checkout for chainlit doc here - https://docs.chainlit.io/get-started/installation\n",
    "\n",
    "# !pip install chainlit \n",
    "#Also if you want to deploy into huggingface space then create a new space and use docker and app file as part of this repo and update it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387330af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
